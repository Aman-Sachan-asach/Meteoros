# Meteoros

*polished version with more feaures to be released over winter break (before 2nd week of Jan)*

![](/images/READMEImages/finalRender.PNG)

## Overview

This project is a real-time cloudscape renderer in Vulkan that was made as the final project for the course, CIS 565: GPU Programming and Architecture, at the University of Pennsylvania. It is based on the cloud system 'NUBIS' that was implemented for the Decima Engine by Guerrilla Games. The clouds were originally made for the game 'Horizon Zero Dawn' and were described in the following SIGGRAPH 2015 and 2017 presentations: 

* 2015 [The Real-time Volumetric Cloudscapes of Horizon Zero Dawn](https://www.guerrilla-games.com/read/the-real-time-volumetric-cloudscapes-of-horizon-zero-dawn)

* 2017 [Nubis: Authoring Realtime Volumetric Cloudscapes with the Decima Engine](https://www.guerrilla-games.com/read/nubis-authoring-real-time-volumetric-cloudscapes-with-the-decima-engine)

Contributors:
1. Aman Sachan - M.S.E. Computer Graphics and Game Technology, UPenn
2. Meghana Seshadri - M.S.E. Computer Graphics and Game Technology, UPenn

Skip Forward to:
1. [Instructions](#Instructions)
2. [Features](#Features)
	- [Current](#Current)
	- [Upcoming](#Upcoming)
3. [Pipeline Overview](#Pipeline)
4. [Implementation Overview](#Implementation)
	- [Rendering](#Rendering)
	- [Modelling](#Modeling)
	- [Lighting](#Lighting)
	- [Post-Processing](#Post)
5. [Optimizations](#Optimizations)
5. [Performance Analysis](#Performance)
6. [Notes](#Notes)
7. [Resources](#Resources)
8. [Bloopers](#Bloopers)

## Instructions <a name="Instructions"></a>

If you wish to run or develop on top of this program, please refer to the [INSTRUCTION.md](https://github.com/Aman-Sachan-asach/Meteoros/blob/master/INSTRUCTION.md) file.

## Features <a name="Features"></a>

### Current <a name="Current"></a>
- Vulkan Framework that is easy to extend, heavily commented, and easy to read and understand
- Multiple Compute and Graphics pipelines working in sync
- Cloud Modelling, Lighting, and Rendering Models as defined by the papers
- HDR color space
- God-Rays and Tone Mapping Post processes
- Raymarching and Cloud rendering Optimizations
- Reprojection Optimization that is slightly buggy and currently only in the test branch 'AmanDev'

### Upcoming <a name="Upcoming"></a>
- Fully functional reprojection optimization
- More refined cloud shapes and lighting
- Anti-Aliasing - Temporal and Fast-Approximate (TXAA and FXAA)
- Offscreen Rendering Pipeline
- Cloud Shadow casting

## Pipeline Overview <a name="Pipeline"></a>

![](/images/READMEImages/pipelinelayout.png)

We have 3 distinct stages in our pipeline: compute stage, rasterization or the regular graphics pipeline stage, and a post-process stage.

#### Compute Stage:
This stage is responsible for the bulk of this project. It handles: 
- Reprojection Compute Shader: Reprojection calculations in a separate compute shader
- Cloud Compute Shader: Cloud raymarching, modeling, lighting, and rendering calculations and stores the result in a HDR color space, i.e. 32bit RGBA channels, texture. This shader also generates a "god-ray creation" texture, which is a grray-scale image used by the god-rays post process shder to create god rays.

#### Synchronization:
The synchronization is in place to ensure that the graphics pipeline doesn't use an image that is half complete and still being written to by the compute pipeline. This is necessary because we are following a compositing model in which we generate the clouds and then paint over them with the rasterized geometry in the world. After this the god rays shader also uses and adds on top of the same texture generated by the compute stage. 

The synchronization point is implemented as a Image Barrier which you can learn more about [here](https://vulkan.lunarg.com/doc/view/1.0.30.0/linux/vkspec.chunked/ch06s05.html#synchronization-memory-barriers).

We don't need a synchronization point between the graphics pipeline stage and the subsequent post-process stage because the commands for these stages are stored in the same Queue which stores the command buffer. All commands in the command buffer attached to a queue is executed in order after the previous command has completely finished executing thus if we store the commands in the command buffer in the correct order, we will not need additional synchronization points.

#### Graphics Pipeline Stage:

This stage is responsible for the rendering of 3D models, which is done via rasterization. The implementation closely follows [this vulkan tutorial](https://vulkan-tutorial.com/) except for the fact that it has been refactored into very readable classes instead of a single file.
This commands for this stage have been commented outis and thus are not being dispatched because they weren't adding anything to our scene.

#### Post Process Stage:

This stage is responsible for adding the god-rays, and tone mapping post-process effects.

## Implementation Overview <a name="Implementation"></a>

### Rendering <a name="Rendering"></a>

We render clouds to a texture using the ray marching technique, which is an image based volumetric rendering technique that is used to evalute volumes as opposed to surfaces. This means that the assumption that a objects properties can be defined at or by its surface are thrown out the window. Ray marching involves sampling a ray at various points along its length because the volume is defined at every point inside itself. 

Ray marching is a widely discussed subject and you can find many great resources to dive into it such as this presentation (https://cis700-procedural-graphics.github.io/files/implicit_surfaces_2_21_17.pdf) from a course on Procedural Graphics at UPenn and [iq's blog](http://www.iquilezles.org/www/articles/raymarchingdf/raymarchingdf.htm)

At every step of our ray march we determine how dense the atmosphere is and if it is dense enough to be quantified as a cloud we light that point. Our lighting model is described later in this readme, however it will make a lot more intuitive sense if one is familiar with volumetic lighting. You can learn more about volumetric lighting in the book [Physically Based Rendering from Theory to Implementation](http://www.pbrt.org/). That is a bit dense and so if you simply want a simple overview go [here](https://www.scratchapixel.com/lessons/advanced-rendering/volume-rendering-for-artists).

To render the sky as a skybox type dome we create 3 spheres, representing the earth, the inner layer of the atmosphere, and the outer layer of the atmosphere.

<img src="/images/READMEImages/layerLayout.png" width="642" height="362"> 

We don't want to render any thing beyond the horizon because we can't see anything beyond the horizon anyway.

<img src="/images/READMEImages/horizonLine.png" width="642" height="362"> 

Placing a camera atop this virtual earth, we can start our actual rendering process. Start raycasting from your camera, for every ray evaluate it at a fixed stepsize when it is inside the the 2 atmosphere layers we just created.

<img src="/images/READMEImages/raymarching.png" width="642" height="362"> 

When we evaluate a point along the ray and determine it has a non-zero density value we know we are inside a cloud.

<img src="/images/READMEImages/InOutOfCloud.png" width="642" height="362">

Now, to actually give this point in the cloud some coloration we can light it by shooting a ray towards our single light source, the sun, and use the resulting energy information to color that point.

<img src="/images/READMEImages/LightCalculationsNaive.png" width="642" height="362">

Cone Sampling is a more efficient way of determining the light energy that will be recieved by that point. Cone sampling involves taking some number of samples from inside the volume of a cone that is aligned with our light source. We take 6 samples using cone sampling and make sure to have the last one be placed relatively far. This far-away sample is a way of taking into account if the cloud and hence point we are trying to light is occluded by another cloud in the distance. Using these 6 samples from within the cone we get a density value which is used to attenuate the light energy reaching the point we are trying to color.

<img src="/images/READMEImages/ConeSampling.png" width="642" height="362"> 

### Modeling <a name="Modeling"></a>

Generating noise on the fly to determine our cloud shape is a very expensive process. This is why we use tiling 3D noise textures with precomputed density values to determine our cloud shape.

There are 3 textures that are used to define the shape of a cloud in this project: 

	3D cloudBaseShapeTexture
	4 channels…
	128^3 resolution…
	The first channel is the Perlin-Worley noise.
	The other 3 channels are Worley noise at increasing frequencies. 
	This 3d texture is used to define the base shape for our clouds.

![](/images/READMEImages/LowFrequencyNoiseChannels.png)

	3D cloudDetailsTexture
	3 channels…
	32^3 resolution…
	Uses Worley noise at increasing frequencies. 
	This texture is used to add detail to the base cloud shape defined by the first 3d noise.

![](/images/highFrequencyDetail.png)

	2D cloudMotionTexture
	3 channels…
	128^2 resolution…
	Uses curl noise. Which is non divergent and is used to fake fluid motion. 
	We use this noise to distort our cloud shapes and add a sense of turbulence.	

![](/images/curlNoise.png)

![](/images/cloudmodelling.png)
<img src="/images/READMEImages/CloudErosion.png" width="642" height="362"> 
![](/images/erodeclouds.png)
![](/images/modellingClouds.png)
![](/images/modellingClouds1.png)

### Lighting <a name="Lighting"></a>

The lighting model as described in the 2017 presentation is an attenuation based lighting model. This means that you start with full intensity, and then reduce it as combination of the following 3 probabilities: 

1. Directional Scattering
2. Absorption / Out-scattering 
3. In-scattering

![](/images/READMEImages/lightingProbs.PNG)

#### Directional Scattering

This retains baseline forward scattering and produces silver lining effects. It is calculated using Henyey-Greenstein equation.

The eccentricity value that generally works well for mid-day sunlight doesn't provide enough bright highlights around the sun during sunset. 

![](/images/READMEImages/hg01.PNG)

Change the eccentricity to have more forward scattering, hence bringing the highlights around the sun. Clouds 90 degrees away from the sun, however, become too dark.

![](/images/READMEImages/hg02.PNG)

To retain baseline forward scattering behavior and get the silver lining highlights, combine 2 HG functions, and factors to control the intensity of this effect as well as its spread away from the sun.

![](/images/READMEImages/hg03.PNG)

![](/images/READMEImages/hg04.PNG)

#### Absorption / Out-scattering

This is the transmittance produced as a result of the Beer-Lambert equation. 

Beer's Law only accounts for attenuation of light and not the emission of light that has in-scattered to the sample point, hence making clouds too dark. 

![](/images/beerslaw.png)

![](/images/READMEImages/beer01.PNG)

By combining 2 Beer-Lambert equations, the attenuation for the second one is reduced to push light further into the cloud.

![](/images/READMEImages/beer02.PNG)

#### In-scattering
This produces the dark edges and bases to the clouds. 

In-scattering is when a light ray that has scattered in a cloud is combined with others on its way to the eye, essentially brightening the region of the cloud you are looking at. In order for this to occur, an area must have a lot of rays scattering into it, which only occurs where there is cloud material. This means that the deeper in the cloud, the more scattering contributors there are, and the amount of in-scattering on the edges of the clouds is lower, which makes them appear dark. Also, since there are no strong scattering sources below clouds, the bottoms of them will have less occurences of in-scattering as well. 

Only attenuation and HG phase: 

![](/images/READMEImages/in01.PNG)

Sampling cloud at low level of density, and accounting for attenuation along in-scatter path. This appears dark because there is little to no in-scattering on the edges.

![](/images/READMEImages/in02.PNG)

Relax the effect over altitude and apply a bias to compensate. 

![](/images/READMEImages/in03.PNG)

Second component accounts for decrease in-scattering over height. 

![](/images/READMEImages/in04.PNG)

### Post Processing <a name="Post"></a>

#### GodRays



#### Tone Mapping



## Optimizations <a name="Optimizations"></a>
Obviously the more samples you take along the ray the better will be the final of your render. T
Ray Sampling Optimization

![](/images/READMEImages/CheapSampling.png)
![](/images/READMEImages/AngularSampling.png)

## Performance Analysis <a name="Performance"></a>

Performance analysis conducted on: Windows 10, i7-7700HQ @ 2.8GHz 32GB, GTX 1070(laptop GPU) 8074MB (Personal Machine: Customized MSI GT62VR 7RE)

## Notes <a name="Notes"></a>
- We did not add checks (which is highly recommended when developing Vulkan code for other users) to make sure some features are supported by the GPU before using them, such as anisotropic filtering and the image formats that the GPU supports.

## Resources <a name="Resources"></a>

#### Texture Resources:
- [Low and High Frequency Noise Textures](https://www.guerrilla-games.com/read/nubis-authoring-real-time-volumetric-cloudscapes-with-the-decima-engine) were made using the 'Nubis Noise Generator' houdini tool that was released along with the 2015 paper. 
- [Curl Noise Textures](http://bitsquid.blogspot.com/2016/07/volumetric-clouds.html)
- Weather Map Texture by Dan Mccan

#### Libraries:
- [Image Loading Library](https://github.com/nothings/stb)
- [Obj Loading Library](https://github.com/syoyo/tinyobjloader)
- [Why to include stb in .cpp file](https://stackoverflow.com/questions/43348798/double-inclusion-and-headers-only-library-stbi-image)
- [Imgui](https://github.com/ocornut/imgui) for our partially wriiten gui

#### Vulkan
- [Vulkan Tutorial](https://vulkan-tutorial.com/)
- [RenderDoc](https://renderdoc.org/)
- [Setting Up Compute Shader that writes to a texture](https://github.com/SaschaWillems/Vulkan/tree/master/examples/raytracing)
- [3D Textures](https://github.com/SaschaWillems/Vulkan/tree/master/examples/texture3d)
- [Pipeline Caching](https://github.com/SaschaWillems/Vulkan/tree/master/examples/radialblur) was used for post-processing and so it made more sense to see how it is done for post processing
- [Radial Blur](https://github.com/SaschaWillems/Vulkan/tree/master/examples/radialblur)

#### Post-Processing:
- [Uncharted 2 Tone Mapping](http://filmicworlds.com/blog/filmic-tonemapping-operators/)
- [God Rays](https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch13.html)

#### Upcoming Feature Set:
- [Off-screen Rendering](https://github.com/SaschaWillems/Vulkan/tree/master/examples/offscreen)
- [Push Constants](https://github.com/SaschaWillems/Vulkan/tree/master/examples/pushconstants)

#### Other Resources
- FBM Procedural Noise Joe Klinger
- Preetham Sun/Sky model from Project Marshmallow 

## Bloopers <a name="Bloopers"></a>

* Tone Mapping Madness

![](/images/READMEImages/meg01.gif)

* Sobel's "edgy" clouds

![](/images/READMEImages/sobeltest.PNG)